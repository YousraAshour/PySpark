{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YousraAshour/PySpark/blob/main/Practical_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05f1e8db",
      "metadata": {
        "id": "05f1e8db"
      },
      "source": [
        "# **Labs 1 and 2 PySpark:**\n",
        "\n",
        "In these labs we will be using the \"[[NeurIPS 2020] Data Science for COVID-19 (DS4C)](https://www.kaggle.com/datasets/kimjihoo/coronavirusdataset?select=PatientInfo.csv)\" dataset, retrieved from [Kaggle](https://www.kaggle.com/) on 1/6/2022, for educational non commercial purpose, License\n",
        "[CC BY-NC-SA 4.0\n",
        "](https://creativecommons.org/licenses/by-nc-sa/4.0/)\n",
        "\n",
        "\n",
        "The csv file that we will be using in this lab is **PatientInfo**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ac9bbf0",
      "metadata": {
        "id": "3ac9bbf0"
      },
      "source": [
        "## PatientInfo.csv\n",
        "\n",
        "**patient_id**\n",
        "the ID of the patient\n",
        "\n",
        "**sex**\n",
        "the sex of the patient\n",
        "\n",
        "**age**\n",
        "the age of the patient\n",
        "\n",
        "**country**\n",
        "the country of the patient\n",
        "\n",
        "**province**\n",
        "the province of the patient\n",
        "\n",
        "**city**\n",
        "the city of the patient\n",
        "\n",
        "**infection_case**\n",
        "the case of infection\n",
        "\n",
        "**infected_by**\n",
        "the ID of who infected the patient\n",
        "\n",
        "\n",
        "**contact_number**\n",
        "the number of contacts with people\n",
        "\n",
        "**symptom_onset_date**\n",
        "the date of symptom onset\n",
        "\n",
        "**confirmed_date**\n",
        "the date of being confirmed\n",
        "\n",
        "**released_date**\n",
        "the date of being released\n",
        "\n",
        "**deceased_date**\n",
        "the date of being deceased\n",
        "\n",
        "**state**\n",
        "isolated / released / deceased"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34b6f619",
      "metadata": {
        "id": "34b6f619"
      },
      "source": [
        "### Import the pyspark and check it's version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29fe2ddc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29fe2ddc",
        "outputId": "a48a30df-8c84-483f-a3b5-30f47bcc512d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 47 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 50.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=949fc4be7df6b7475718e927721aef103152653ab9c8055a5e3feb96e30e1a8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ],
      "source": [
        "! pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70c0fc8e",
      "metadata": {
        "id": "70c0fc8e"
      },
      "source": [
        "### Import and create SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94904228",
      "metadata": {
        "id": "94904228"
      },
      "outputs": [],
      "source": [
        "import pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "Dwb7V3xuksd8"
      },
      "id": "Dwb7V3xuksd8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "apmfA3K3laMI"
      },
      "id": "apmfA3K3laMI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7356d68d",
      "metadata": {
        "id": "7356d68d"
      },
      "source": [
        "### Load the PatientInfo.csv file and show the first 5 rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5185728",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "d5185728",
        "outputId": "6411524e-38c6-4d3a-d251-83770de2ad9b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>pre { white-space: pre !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display, HTML\n",
        "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2196bca5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "2196bca5",
        "outputId": "f69ddb4d-58d5-4b7c-d99d-d8f14cdec5dc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e7034d7af67b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/PatientInfo.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/content/PatientInfo.csv"
          ]
        }
      ],
      "source": [
        "df = spark.read.csv(\"/content/PatientInfo.csv\", header=True, inferSchema=True)\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d43bdca",
      "metadata": {
        "id": "4d43bdca"
      },
      "source": [
        "### Display the schema of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c51a2507",
      "metadata": {
        "id": "c51a2507"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3114e1dd",
      "metadata": {
        "id": "3114e1dd"
      },
      "source": [
        "### Display the statistical summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18d63947",
      "metadata": {
        "id": "18d63947"
      },
      "outputs": [],
      "source": [
        "df.summary().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c78859b9",
      "metadata": {
        "id": "c78859b9"
      },
      "source": [
        "### Using the state column.\n",
        "### How many people survived (released), and how many didn't survive (isolated/deceased)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63e84045",
      "metadata": {
        "id": "63e84045"
      },
      "outputs": [],
      "source": [
        "df.groupBy(\"state\").count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59c16346",
      "metadata": {
        "id": "59c16346"
      },
      "source": [
        "### Display the number of null values in each column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4adc0ab6",
      "metadata": {
        "id": "4adc0ab6"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col,isnan,when,count\n",
        "df.select([count(when(col(c).isNull(),c)).alias(c)\n",
        "                    for c in df.columns]).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3133f5a",
      "metadata": {
        "id": "c3133f5a"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec32fa0c",
      "metadata": {
        "id": "ec32fa0c"
      },
      "source": [
        "### Fill the nulls in the deceased_date with the released_date. \n",
        "- You can use <b>coalesce</b> function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5ba64f9",
      "metadata": {
        "id": "d5ba64f9"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import coalesce\n",
        "df = df.withColumn('deceased_date',coalesce(df[\"released_date\"],\n",
        "                                               df[\"deceased_date\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ec324f5",
      "metadata": {
        "id": "4ec324f5"
      },
      "source": [
        "### Add a column named no_days which is difference between the deceased_date and the confirmed_date then show the top 5 rows. Print the schema.\n",
        "- <b> Hint: You need to typecast these columns as date first <b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d065e472",
      "metadata": {
        "id": "d065e472"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import DateType"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"deceased_date\",col(\"deceased_date\").cast(DateType()))\n",
        "df = df.withColumn(\"confirmed_date\",col(\"confirmed_date\").cast(DateType()))"
      ],
      "metadata": {
        "id": "booyjFkbqAGV"
      },
      "id": "booyjFkbqAGV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"deceased_date\").show()"
      ],
      "metadata": {
        "id": "QoTF-unvqBuY"
      },
      "id": "QoTF-unvqBuY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df.withColumn('no_days',(df[\"deceased_date_updated\"] - df[\"confirmed_date\"])).show()\n",
        "from pyspark.sql.functions import datediff \n",
        "df = df.withColumn(\"no_days\", datediff(col(\"deceased_date\"),col(\"confirmed_date\")))\n",
        "df.show()"
      ],
      "metadata": {
        "id": "D1ncXr_WqLZI"
      },
      "id": "D1ncXr_WqLZI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "967b2466",
      "metadata": {
        "id": "967b2466"
      },
      "source": [
        "### Add a is_male column if male then it should yield true, else then False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8efe03b5",
      "metadata": {
        "id": "8efe03b5"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import when\n",
        "df = df.withColumn(\"is_male\",when(col(\"sex\") == 'male','true').otherwise('false'))\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8603d7c0",
      "metadata": {
        "id": "8603d7c0"
      },
      "source": [
        "### Add a is_dead column if patient state is not released then it should yield true, else then False\n",
        "\n",
        "- Use <b>UDF</b> to perform this task. \n",
        "- However, UDF is not recommended there is no built in function can do the required operation.\n",
        "- UDF is slower than built in functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3343075",
      "metadata": {
        "id": "d3343075"
      },
      "outputs": [],
      "source": [
        "def statrel(stat):\n",
        "  x=False\n",
        "  if stat != 'released':\n",
        "    x=True\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "convertUDF = udf(lambda z: statrel(z),StringType())"
      ],
      "metadata": {
        "id": "23UC6B_gqW85"
      },
      "id": "23UC6B_gqW85",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"is_dead\",convertUDF(col(\"state\")))\n",
        "df.show()"
      ],
      "metadata": {
        "id": "T3CFjoVNqXfb"
      },
      "id": "T3CFjoVNqXfb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e15264a9",
      "metadata": {
        "id": "e15264a9"
      },
      "source": [
        "### Change the ages to bins from 10s, 0s, 10s, 20s,.etc to 0,10, 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a2b57ed",
      "metadata": {
        "id": "7a2b57ed"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import translate\n",
        "from pyspark.sql.types import IntegerType\n",
        "df = df.withColumn('age', translate('age','s',''))\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43cdf846",
      "metadata": {
        "id": "43cdf846"
      },
      "source": [
        "### Change age, and no_days  to be typecasted as Double"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfea7fcb",
      "metadata": {
        "id": "dfea7fcb"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import IntegerType , DoubleType\n",
        "df = df.withColumn('age', col('age').cast(DoubleType()))\n",
        "df = df.withColumn('no_days', col('no_days').cast(DoubleType()))\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d31e3a87",
      "metadata": {
        "id": "d31e3a87"
      },
      "source": [
        "### Drop the columns\n",
        "[\"patient_id\",\"sex\",\"infected_by\",\"contact_number\",\"released_date\",\"state\",\n",
        "\"symptom_onset_date\",\"confirmed_date\",\"deceased_date\",\"country\",\"no_days\",\n",
        "\"city\",\"infection_case\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ae0c428",
      "metadata": {
        "id": "3ae0c428"
      },
      "outputs": [],
      "source": [
        "cols = (\"patient_id\",\"sex\",\"infected_by\",\"contact_number\",\"released_date\",\"state\", \"symptom_onset_date\",\"confirmed_date\",\"deceased_date\",\"country\",\"no_days\", \"city\",\"infection_case\")\n",
        "df = df.drop(*cols)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32df47a6",
      "metadata": {
        "id": "32df47a6"
      },
      "source": [
        "### Recount the number of nulls now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d66eaa7",
      "metadata": {
        "id": "6d66eaa7"
      },
      "outputs": [],
      "source": [
        "df.select([count(when(col(c).isNull(),c)).alias(c)\n",
        "                    for c in df.columns])\\\n",
        "                    .show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc71f158",
      "metadata": {
        "id": "cc71f158"
      },
      "source": [
        "## Now do the same but using SQL select statement"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d540282",
      "metadata": {
        "id": "1d540282"
      },
      "source": [
        "### From the original Patient DataFrame, Create a temporary view (table)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf56099f",
      "metadata": {
        "id": "cf56099f"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb004277",
      "metadata": {
        "id": "cb004277"
      },
      "source": [
        "### Use SELECT statement to select all columns from the dataframe and show the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12ae8226",
      "metadata": {
        "scrolled": false,
        "id": "12ae8226"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0da4318",
      "metadata": {
        "id": "e0da4318"
      },
      "source": [
        "### *Using SQL commands*, limit the output to only 5 rows "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97911bdd",
      "metadata": {
        "id": "97911bdd"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b70edf17",
      "metadata": {
        "id": "b70edf17"
      },
      "source": [
        "### Select the count of males and females in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00e9fd65",
      "metadata": {
        "id": "00e9fd65"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73683674",
      "metadata": {
        "id": "73683674"
      },
      "source": [
        "### How many people did survive, and how many didn't?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e5a3908",
      "metadata": {
        "id": "1e5a3908"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4424228",
      "metadata": {
        "id": "d4424228"
      },
      "source": [
        "### Now, let's perform some preprocessing using SQL:\n",
        "1. Convert *age* column to double after removing the 's' at the end -- *hint: check SUBSTRING method*\n",
        "2. Select only the following columns: `['sex', 'age', 'province', 'state']`\n",
        "3. Store the result of the query in a new dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16994e4d",
      "metadata": {
        "id": "16994e4d"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3cf4f0f",
      "metadata": {
        "scrolled": true,
        "id": "d3cf4f0f"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "791ec1b8",
      "metadata": {
        "id": "791ec1b8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61c70bc8",
      "metadata": {
        "id": "61c70bc8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4a24c24",
      "metadata": {
        "id": "d4a24c24"
      },
      "source": [
        "## Machine Learning \n",
        "### Create a pipeline model to predict is_dead and evaluate the performance.\n",
        "- Use <b>StringIndexer</b> to transform <b>string</b> data type to indices.\n",
        "- Use <b>OneHotEncoder</b> to deal with categorical values.\n",
        "- Use <b>Imputer</b> to fill missing data with mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a71806f",
      "metadata": {
        "id": "7a71806f"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# indexer = StringIndexer().setInputCol(\"is_dead\").setOutputCol(\"is_dead1\")\n",
        "# df = indexer.fit(df).transform(df)\n",
        "# df.show()"
      ],
      "metadata": {
        "id": "WotBJJgFqodh"
      },
      "id": "WotBJJgFqodh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = StringIndexer().setInputCol(\"is_male\").setOutputCol(\"is_male1\").fit(df).transform(df)\n",
        "# df.show()"
      ],
      "metadata": {
        "id": "d_NctImxqp7L"
      },
      "id": "d_NctImxqp7L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = StringIndexer().setInputCol(\"province\").setOutputCol(\"province1\").fit(df).transform(df)\n",
        "# df.show()"
      ],
      "metadata": {
        "id": "T4p0NnaMqrSc"
      },
      "id": "T4p0NnaMqrSc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cols = (\"province\",\"is_male\",'is_dead')\n",
        "# df = df.drop(*cols)\n",
        "# df.show()"
      ],
      "metadata": {
        "id": "itda30ptqtZe"
      },
      "id": "itda30ptqtZe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = [\"province\",\"is_male\"]\n",
        "\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
        "    for c in cols\n",
        "]\n",
        "\n",
        "encoders = [\n",
        "    OneHotEncoder(\n",
        "        inputCol=indexer.getOutputCol(),\n",
        "        outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n",
        "    for indexer in indexers\n",
        "]\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[encoder.getOutputCol() for encoder in encoders],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "\n",
        "pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
        "pipeline.fit(df).transform(df).show()"
      ],
      "metadata": {
        "id": "-yFrRNtcqu0o"
      },
      "id": "-yFrRNtcqu0o",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "Practical_Day_1_DF_SQL_ML_Student.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}